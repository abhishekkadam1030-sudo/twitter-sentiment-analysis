{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7f11884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (1276080, 40) (1276080,)\n"
     ]
    }
   ],
   "source": [
    "#tokenizer_padding\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Load train/val/test\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Tokenizer\n",
    "MAX_VOCAB = 50000  # cap vocab size\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_df[\"clean_text\"])\n",
    "\n",
    "# Convert to sequences\n",
    "def to_seq(df, tokenizer, maxlen=40):\n",
    "    seqs = tokenizer.texts_to_sequences(df[\"clean_text\"])\n",
    "    return pad_sequences(seqs, maxlen=maxlen, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "X_train = to_seq(train_df, tokenizer)\n",
    "X_test = to_seq(test_df, tokenizer)\n",
    "\n",
    "y_train = train_df[\"label\"].values\n",
    "y_test = test_df[\"label\"].values\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, y_train.shape)\n",
    "\n",
    "# Save tokenizer for reuse\n",
    "with open(\"tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "# Save numpy arrays\n",
    "np.save(\"X_train.npy\", X_train)\n",
    "np.save(\"y_train.npy\", y_train)\n",
    "np.save(\"X_test.npy\", X_test)\n",
    "np.save(\"y_test.npy\", y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc25a7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 40, 100)           5000000   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128)               117248    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5117506 (19.52 MB)\n",
      "Trainable params: 5117506 (19.52 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "11485/11485 [==============================] - 1498s 130ms/step - loss: 0.4743 - accuracy: 0.7689 - val_loss: 0.4402 - val_accuracy: 0.7939\n",
      "Epoch 2/10\n",
      "11485/11485 [==============================] - 1575s 137ms/step - loss: 0.4181 - accuracy: 0.8081 - val_loss: 0.4356 - val_accuracy: 0.7963\n",
      "Epoch 3/10\n",
      "11485/11485 [==============================] - 1685s 147ms/step - loss: 0.3864 - accuracy: 0.8260 - val_loss: 0.4444 - val_accuracy: 0.7933\n",
      "Epoch 4/10\n",
      "11485/11485 [==============================] - 4219s 367ms/step - loss: 0.3535 - accuracy: 0.8436 - val_loss: 0.4661 - val_accuracy: 0.7883\n",
      "Epoch 5/10\n",
      "11485/11485 [==============================] - 1754s 153ms/step - loss: 0.3196 - accuracy: 0.8605 - val_loss: 0.5076 - val_accuracy: 0.7811\n",
      "Test accuracy: 0.7960973978042603\n"
     ]
    }
   ],
   "source": [
    "# src/08_lstm_baseline.py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load preprocessed arrays\n",
    "X_train = np.load(\"X_train.npy\")\n",
    "y_train = np.load(\"y_train.npy\")\n",
    "X_test = np.load(\"X_test.npy\")\n",
    "y_test = np.load(\"y_test.npy\")\n",
    "\n",
    "# Convert labels to one-hot\n",
    "num_classes = 2\n",
    "y_train_cat = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test_cat = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# Model parameters\n",
    "MAX_VOCAB = 50000\n",
    "EMBED_DIM = 100\n",
    "MAXLEN = 40\n",
    "\n",
    "# Build model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=MAX_VOCAB, output_dim=EMBED_DIM, input_length=MAXLEN),\n",
    "    LSTM(128, return_sequences=False),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation=\"softmax\")\n",
    "])\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Train\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train_cat,\n",
    "    epochs=10,\n",
    "    validation_split = 0.1,\n",
    "    batch_size=100,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test_cat, verbose=0)\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac4f1016",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Save model after training\n",
    "model.save(\"lstm_baseline.h5\")   # HDF5 format\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3267ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
