{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a45b57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   target          id                          date     query  \\\n",
      "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
      "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
      "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
      "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
      "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
      "\n",
      "              user                                               text  \n",
      "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
      "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
      "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
      "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
      "4           Karoli  @nationwideclass no, it's not behaving at all....  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1600000 entries, 0 to 1599999\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count    Dtype \n",
      "---  ------  --------------    ----- \n",
      " 0   target  1600000 non-null  int64 \n",
      " 1   id      1600000 non-null  int64 \n",
      " 2   date    1600000 non-null  object\n",
      " 3   query   1600000 non-null  object\n",
      " 4   user    1600000 non-null  object\n",
      " 5   text    1600000 non-null  object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 73.2+ MB\n",
      "None\n",
      "             target            id\n",
      "count  1.600000e+06  1.600000e+06\n",
      "mean   2.000000e+00  1.998818e+09\n",
      "std    2.000001e+00  1.935761e+08\n",
      "min    0.000000e+00  1.467810e+09\n",
      "25%    0.000000e+00  1.956916e+09\n",
      "50%    2.000000e+00  2.002102e+09\n",
      "75%    4.000000e+00  2.177059e+09\n",
      "max    4.000000e+00  2.329206e+09\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "COLS = [\"target\", \"id\", \"date\", \"query\", \"user\", \"text\"]\n",
    "\n",
    "df = pd.read_csv(\"abhi.csv\", encoding=\"latin-1\", names=COLS)\n",
    "\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0eeaa66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "missing = df.isnull().sum().sort_values(ascending=False)\n",
    "print(missing[missing > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1b5f1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#irrelevent columns\n",
    "drop_cols = [\n",
    "    'id',\n",
    "    'date',\n",
    "    'query',\n",
    "    'user',\n",
    "]\n",
    "\n",
    "df.drop(columns=drop_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0f66846",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"target\"] = df[\"target\"].map({0:0, 4:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5beb7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"abhi_minimal.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3d318a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"target\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1664ba12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"abhi_minimal.csv\")\n",
    "df = df.rename(columns={\"target\": \"label\"})\n",
    "\n",
    "df.to_csv(\"sentiment140_labeled.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40806743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    800000\n",
       "1    800000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"label\"].unique()\n",
    "df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a83eeb05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "100%|██████████| 1600000/1600000 [00:59<00:00, 26809.79it/s]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "nltk.download(\"stopwords\")\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "URL_PATTERN = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "MENTION_PATTERN = re.compile(r\"@\\w+\")\n",
    "HASHTAG_PATTERN = re.compile(r\"#(\\w+)\")\n",
    "RT_PATTERN = re.compile(r\"\\brt\\b\", re.IGNORECASE)\n",
    "NON_ALNUM_PATTERN = re.compile(r\"[^a-z0-9\\s\\.\\,\\!\\?\\']\")\n",
    "\n",
    "\n",
    "def clean_tweet(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    t = text.strip()\n",
    "\n",
    "    # Remove URLs and mentions\n",
    "    t = URL_PATTERN.sub(\" \", t)\n",
    "    \n",
    "    t = MENTION_PATTERN.sub(\" \", t)\n",
    "\n",
    "    # Keep hashtag words (drop '#')\n",
    "    t = HASHTAG_PATTERN.sub(r\"\\1\", t)\n",
    "\n",
    "\n",
    "    # Remove RT markers\n",
    "    t = RT_PATTERN.sub(\" \", t)\n",
    "\n",
    "    # Lowercase\n",
    "    t = t.lower()\n",
    "\n",
    "\n",
    "    # Remove special characters (keep basic punctuation)\n",
    "    t = NON_ALNUM_PATTERN.sub(\" \", t)\n",
    "\n",
    "    # Normalize whitespace\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "\n",
    "    # Remove stopwords (light)\n",
    "    tokens = [w for w in t.split() if w not in STOPWORDS]\n",
    "    t = \" \".join(tokens)\n",
    "\n",
    "    return t\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv(\"sentiment140_labeled.csv\")\n",
    "    df[\"clean_text\"] = df[\"text\"].progress_apply(clean_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "319bd7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty rows in 'text': 4900\n"
     ]
    }
   ],
   "source": [
    "# Count empty rows in the 'text' column\n",
    "empty_rows = (df[\"clean_text\"].str.strip() == \"\").sum()\n",
    "print(\"Empty rows in 'text':\", empty_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8b0af62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: sentiment140_clean.csv Shape: (1595100, 3)\n"
     ]
    }
   ],
   "source": [
    "# Drop empty rows post-cleaning\n",
    "df = df[df[\"clean_text\"].str.len() > 0].copy()\n",
    "\n",
    "\n",
    "df.to_csv(\"sentiment140_clean.csv\", index=False)\n",
    "print(\"Saved:\", \"sentiment140_clean.csv\", \"Shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8125418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train {1: 0.5, 0: 0.5}\n",
      "test {1: 0.5, 0: 0.5}\n"
     ]
    }
   ],
   "source": [
    "#train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "df = pd.read_csv(\"sentiment140_clean.csv\")\n",
    "\n",
    "# 80/20 stratified split\n",
    "train_df, test_df = train_test_split(\n",
    "    df, test_size=0.2, random_state=42, stratify=df[\"label\"]\n",
    ")\n",
    "\n",
    "# Save and check distribution\n",
    "for name, part in [(\"train\", train_df), (\"test\", test_df)]:\n",
    "    part.to_csv(f\"{name}.csv\", index=False)\n",
    "    print(name, part[\"label\"].value_counts(normalize=True).round(3).to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0728cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8018    0.7744    0.7879    159492\n",
      "           1     0.7819    0.8086    0.7950    159528\n",
      "\n",
      "    accuracy                         0.7915    319020\n",
      "   macro avg     0.7918    0.7915    0.7914    319020\n",
      "weighted avg     0.7918    0.7915    0.7914    319020\n",
      "\n",
      "Confusion matrix (test):\n",
      "[[123510  35982]\n",
      " [ 30531 128997]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "X_train, y_train = train_df[\"clean_text\"], train_df[\"label\"]\n",
    "X_test, y_test = test_df[\"clean_text\"], test_df[\"label\"]\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        ngram_range=(1,3),  # capture bigrams\n",
    "        min_df=5,\n",
    "        max_df=0.9,\n",
    "        sublinear_tf=True\n",
    "    )),\n",
    "    (\"clf\", LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        class_weight=\"balanced\",\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "print(\"Test report:\")\n",
    "y_pred = pipe.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "print(\"Confusion matrix (test):\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "311d184d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 1276080\n",
      "test data: 319020\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data:\", X_train.shape[0])\n",
    "print(\"test data:\",X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f6da780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "with open('pipe.pkl','wb')as f:\n",
    "    pk.dump(pipe, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5e6ca3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
